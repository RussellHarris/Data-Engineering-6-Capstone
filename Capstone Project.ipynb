{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# US Visitor Demographics by State\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The goal of this project was to create an ETL pipeline that joins i94 Immigration data with U.S. Demographics, then enriches the data with historical temperature data and airport codes. This aggregation allows us to see if any of the following impact immigration patterns:\n",
    "\n",
    " * Warmer or cooler temperatures.\n",
    " * Population demographics of people.\n",
    " \n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Quick start\n",
    "1. Edit 'dwh.cfg' and add your 'AWS Key', 'AWS Secret', 'S3 Bucket'.\n",
    "2. Run 'stage_to_s3.py' to upload data to your S3 instance.\n",
    "3. Run 'IaC.ipynb' to create your RedShift instance.\n",
    "4. Run 'etl.py' to stage and ingest the data to RedShift.\n",
    "![etl_py_success](./images/etl_py_success.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Param</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOST</td>\n",
       "      <td>dwhcluster.cck8vmzifioi.us-west-2.redshift.ama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DB_NAME</td>\n",
       "      <td>dwhcapstone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DB_USER</td>\n",
       "      <td>dwhuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DB_PASSWORD</td>\n",
       "      <td>Passw0rd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DB_PORT</td>\n",
       "      <td>5439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ARN_IAM_ROLE</td>\n",
       "      <td>'arn:aws:iam::473886897808:role/dwhRole'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>S3_BUCKET</td>\n",
       "      <td>udacity-dend-capstone-nj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>S3_RAW_DATA</td>\n",
       "      <td>s3a://udacity-dend-capstone-nj/raw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Param                                              Value\n",
       "0          HOST  dwhcluster.cck8vmzifioi.us-west-2.redshift.ama...\n",
       "1       DB_NAME                                        dwhcapstone\n",
       "2       DB_USER                                            dwhuser\n",
       "3   DB_PASSWORD                                           Passw0rd\n",
       "4       DB_PORT                                               5439\n",
       "5  ARN_IAM_ROLE           'arn:aws:iam::473886897808:role/dwhRole'\n",
       "6     S3_BUCKET                           udacity-dend-capstone-nj\n",
       "7   S3_RAW_DATA                 s3a://udacity-dend-capstone-nj/raw"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports and installs\n",
    "import boto3\n",
    "import configparser\n",
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from pyspark.sql import SparkSession\n",
    "from sql_queries import drop_table_queries \\\n",
    "                      , create_table_queries \\\n",
    "                      , copy_table_queries \\\n",
    "                      , insert_table_queries \\\n",
    "                      , staging_checks \\\n",
    "                      , insert_checks\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['KEY']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['SECRET']\n",
    "\n",
    "HOST = config.get(\"CLUSTER\", \"HOST\")\n",
    "DB_NAME = config.get(\"CLUSTER\", \"DB_NAME\")\n",
    "DB_USER = config.get(\"CLUSTER\", \"DB_USER\")\n",
    "DB_PASSWORD = config.get(\"CLUSTER\", \"DB_PASSWORD\")\n",
    "DB_PORT = config.get(\"CLUSTER\", \"DB_PORT\")\n",
    "ARN_IAM_ROLE = config.get(\"IAM_ROLE\", \"arn\")\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                    region_name=\"us-west-2\"\n",
    "                   )\n",
    "\n",
    "s3_bucket = config.get(\"S3\",\"BUCKET\")\n",
    "s3_raw_data = 's3a://' + s3_bucket + '/raw'\n",
    "\n",
    "pd.DataFrame({\"Param\":\n",
    "                  [\"HOST\", \"DB_NAME\", \"DB_USER\", \"DB_PASSWORD\", \"DB_PORT\", \"ARN_IAM_ROLE\", \"S3_BUCKET\", \"S3_RAW_DATA\"],\n",
    "              \"Value\":\n",
    "                  [HOST, DB_NAME, DB_USER, DB_PASSWORD, DB_PORT, ARN_IAM_ROLE, s3_bucket, s3_raw_data]\n",
    "             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "This ETL will extract raw data from S3 into staging tables in RedShift. The data will then be transformed and loaded into the final fact and dimension tables to allow for immigration data analysis.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "* **I94 Immigration Data:** This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. Source: US National Tourism and Trade Office https://travel.trade.gov/research/reports/i94/historical/2016.html See **/raw_data/I94_SAS_Labels_Descriptions.SAS** for a detailed description of the columns.\n",
    "\n",
    "* **World Temperature Data:** This dataset came from Kaggle.\n",
    "\n",
    "* **U.S. City Demographic Data:** This data comes from OpenSoft.\n",
    "\n",
    "* **Airport Code Table:** This is a simple table of airport codes and corresponding cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Load data to S3 (Optional after first run)\n",
    "\n",
    "The below code only needs to be run once to prepare your S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0,saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_jan16_sub =spark.read.format('com.github.saurfang.sas.spark').load('/data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat')\n",
    "i94_feb16_sub =spark.read.format('com.github.saurfang.sas.spark').load('/data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat')\n",
    "i94_mar16_sub =spark.read.format('com.github.saurfang.sas.spark').load('/data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat')\n",
    "i94_apr16_sub =spark.read.format('com.github.saurfang.sas.spark').load('/data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "i94_may16_sub =spark.read.format('com.github.saurfang.sas.spark').load('/data/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat')\n",
    "i94_jun16_sub =spark.read.format('com.github.saurfang.sas.spark').load('/data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat')\n",
    "i94_jul16_sub =spark.read.format('com.github.saurfang.sas.spark').load('/data/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat')\n",
    "i94_aug16_sub =spark.read.format('com.github.saurfang.sas.spark').load('/data/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat')\n",
    "i94_sep16_sub =spark.read.format('com.github.saurfang.sas.spark').load('/data/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat')\n",
    "i94_oct16_sub =spark.read.format('com.github.saurfang.sas.spark').load('/data/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat')\n",
    "i94_nov16_sub =spark.read.format('com.github.saurfang.sas.spark').load('/data/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat')\n",
    "i94_dec16_sub =spark.read.format('com.github.saurfang.sas.spark').load('/data/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['validres','delete_days','delete_mexl','delete_dup','delete_visa','delete_recdup']\n",
    "i94_jun16_sub = i94_jun16_sub.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload of i94_jan16_sub...complete\n",
      "Upload of i94_feb16_sub...complete\n",
      "Upload of i94_mar16_sub...complete\n",
      "Upload of i94_apr16_sub...complete\n",
      "Upload of i94_may16_sub...complete\n",
      "Upload of i94_jun16_sub...complete\n",
      "Upload of i94_jul16_sub...complete\n",
      "Upload of i94_aug16_sub...complete\n",
      "Upload of i94_sep16_sub...complete\n",
      "Upload of i94_oct16_sub...complete\n",
      "Upload of i94_nov16_sub...complete\n",
      "Upload of i94_dec16_sub...complete\n"
     ]
    }
   ],
   "source": [
    "df_i94 = [i94_jan16_sub,\n",
    "          i94_feb16_sub,\n",
    "          i94_mar16_sub,\n",
    "          i94_apr16_sub,\n",
    "          i94_may16_sub,\n",
    "          i94_jun16_sub,\n",
    "          i94_jul16_sub,\n",
    "          i94_aug16_sub,\n",
    "          i94_sep16_sub,\n",
    "          i94_oct16_sub,\n",
    "          i94_nov16_sub,\n",
    "          i94_dec16_sub]\n",
    "\n",
    "df_i94_names = ['i94_jan16_sub',\n",
    "                'i94_feb16_sub',\n",
    "                'i94_mar16_sub',\n",
    "                'i94_apr16_sub',\n",
    "                'i94_may16_sub',\n",
    "                'i94_jun16_sub',\n",
    "                'i94_jul16_sub',\n",
    "                'i94_aug16_sub',\n",
    "                'i94_sep16_sub',\n",
    "                'i94_oct16_sub',\n",
    "                'i94_nov16_sub',\n",
    "                'i94_dec16_sub']\n",
    "\n",
    "count=0\n",
    "for i,df in enumerate(df_i94):\n",
    "    df.write.mode('ignore').parquet(s3_raw_data + '/i94_immigration_data/' + df_i94_names[count])\n",
    "    print('Upload of ' + df_i94_names[count] + '...complete')\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "s3.meta.client.upload_file('/data2/GlobalLandTemperaturesByCity.csv', s3_bucket, 'raw/world_temperature_data/GlobalLandTemperaturesByCity.csv')\n",
    "s3.meta.client.upload_file('./raw_data/us-cities-demographics.csv', s3_bucket, 'raw/us_city_demographic_data/us-cities-demographics.csv')\n",
    "s3.meta.client.upload_file('./raw_data/airport-codes_csv.csv', s3_bucket, 'raw/airport_code_data/airport-codes_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "s3.meta.client.upload_file('./lookup_data/i94addrl.csv', s3_bucket, 'lookup/i94addrl.csv')\n",
    "s3.meta.client.upload_file('./lookup_data/i94cntyl.csv', s3_bucket, 'lookup/i94cntyl.csv')\n",
    "s3.meta.client.upload_file('./lookup_data/i94model.csv', s3_bucket, 'lookup/i94model.csv')\n",
    "s3.meta.client.upload_file('./lookup_data/i94prtl.csv', s3_bucket, 'lookup/i94prtl.csv')\n",
    "s3.meta.client.upload_file('./lookup_data/i94prtl_enriched.csv', s3_bucket, 'lookup/i94prtl_enriched.csv')\n",
    "s3.meta.client.upload_file('./lookup_data/i94visal.csv', s3_bucket, 'lookup/i94visal.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "\n",
    "#### Cleaning Steps\n",
    "**Airport Code Table**\n",
    "1. state_cd - New field derived from 'iso_region'.\n",
    "2. lattitude - New field derived from 'coordinates'.\n",
    "3. longitude - New field derived from 'coordinates'.\n",
    "\n",
    "**I94 Immigration Data**\n",
    "1. i94cit - Normalize values not found in look up table to '999'.\n",
    "2. i94res - Normalize values not found in look up table to '999'.\n",
    "3. i94port - Normalize values not found in look up table to 'XXX'.\n",
    "4. i94mode - Normalize values not found in look up table to '9'.\n",
    "5. i94addr - Normalize values not found in look up table to '99'.\n",
    "6. i94visa - Normalize values not found in look up table to '2'.\n",
    "\n",
    "**U.S. City Demographic Data:**\n",
    "1. 'American Indian and Alaska Native' - Derived from 'count'.\n",
    "2. 'Asian' - Derived from 'count'.\n",
    "3. 'Black or African-American' - Derived from 'count'.\n",
    "4. 'Hispanic or Latino' - Derived from 'count'.\n",
    "5. 'White' - Derived from 'count'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "![table_diagram](./images/table_diagram.png)\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "1. Upload data to S3.\n",
    "2. Load data from S3 to RedShift Staging Tables.\n",
    "3. Copy data from RedShift Staging Tables to Fact and Dimension Tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data and perform Data Quality Checks\n",
    "#### 4.1 Create the data model\n",
    "See 'sql_queries.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def drop_tables(cur, conn):\n",
    "    for query in drop_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "def create_tables(cur, conn):\n",
    "    for query in create_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "def load_staging_tables(cur, conn):\n",
    "    for query in copy_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "def insert_tables(cur, conn):\n",
    "    for query in insert_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "        \n",
    "def quality_checks(cur, conn, checks):\n",
    "    passed_count = 0\n",
    "    failed_count = 0\n",
    "    passed_tests = []\n",
    "    failed_tests = []\n",
    "\n",
    "    for check in checks:\n",
    "        chk_sql = check.get('check_sql')\n",
    "        exp_result = check.get('expected_result')\n",
    "\n",
    "        cur.execute(chk_sql)\n",
    "        result = cur.fetchall()\n",
    "    \n",
    "        if exp_result != result[0][0]:\n",
    "            failed_count += 1\n",
    "            failed_tests.append((chk_sql, exp_result))\n",
    "        else:\n",
    "            passed_count += 1\n",
    "            passed_tests.append((chk_sql, exp_result))\n",
    "        \n",
    "    if passed_count > 0:\n",
    "        print(\"PASSED QUALITY CHECKS:\")\n",
    "        for passed_test in passed_tests:\n",
    "            print(passed_test)\n",
    "        \n",
    "    if failed_count > 0:\n",
    "        print(\"FAILED QUALITY CHECKS:\")\n",
    "        for failed_test in failed_tests:\n",
    "            print(failed_test)\n",
    "        raise Exception('Staging Data check FAILED!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin ETL:\n",
      "\n",
      "drop_tables...COMPLETE\n",
      "\n",
      "create_tables...COMPLETE\n",
      "CPU times: user 21.6 ms, sys: 5.45 ms, total: 27.1 ms\n",
      "Wall time: 1min 57s\n",
      "\n",
      "load_staging_tables...COMPLETE\n",
      "\n",
      "PASSED QUALITY CHECKS:\n",
      "('SELECT COUNT(*) FROM staging_airport_codes', 55075)\n",
      "('SELECT COUNT(*) FROM staging_i94_immigration', 40790529)\n",
      "('SELECT COUNT(*) FROM staging_us_city_demographics', 2891)\n",
      "('SELECT COUNT(*) FROM staging_world_temperatures', 8599212)\n",
      "('SELECT COUNT(*) FROM i94addrl', 55)\n",
      "('SELECT COUNT(*) FROM i94cntyl', 289)\n",
      "('SELECT COUNT(*) FROM i94model', 4)\n",
      "('SELECT COUNT(*) FROM i94prtl', 697)\n",
      "('SELECT COUNT(*) FROM i94visal', 3)\n",
      "\n",
      "staging_quality_checks...COMPLETE\n",
      "CPU times: user 16.3 ms, sys: 3.61 ms, total: 19.9 ms\n",
      "Wall time: 1min 29s\n",
      "\n",
      "insert_tables...COMPLETE\n",
      "\n",
      "PASSED QUALITY CHECKS:\n",
      "('SELECT COUNT(*) FROM airport_codes WHERE ident IS NULL', 0)\n",
      "('SELECT COUNT(*) FROM i94_immigration', 40790529)\n",
      "('SELECT COUNT(*) FROM us_city_demographics WHERE state_code IS NULL', 0)\n",
      "('SELECT COUNT(*) FROM world_temperatures', 8599212)\n",
      "('SELECT COUNT(*) FROM us_state_visitor_demographics WHERE state_code IS NULL', 0)\n",
      "\n",
      "insert_quality_checks...COMPLETE\n",
      "\n",
      "End of ETL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\"\\\n",
    "                        .format(HOST, DB_NAME, DB_USER, DB_PASSWORD, DB_PORT))\n",
    "cur = conn.cursor()\n",
    "\n",
    "print('Begin ETL:')\n",
    "\n",
    "drop_tables(cur, conn)\n",
    "print('\\n' + 'drop_tables...COMPLETE')\n",
    "\n",
    "create_tables(cur, conn)\n",
    "print('\\n' + 'create_tables...COMPLETE')\n",
    "\n",
    "%time load_staging_tables(cur, conn) #Approximate Staging Time: 2min for ~50 Million Records with 2 Nodes\n",
    "print('\\n' + 'load_staging_tables...COMPLETE')\n",
    "\n",
    "print('')\n",
    "quality_checks(cur, conn, staging_checks)\n",
    "print('\\n' + 'staging_quality_checks...COMPLETE')\n",
    "\n",
    "%time insert_tables(cur, conn) #Approximate Insert Time: 1min for ~50 Million Records with 2 Nodes\n",
    "print('\\n' + 'insert_tables...COMPLETE')\n",
    "\n",
    "print('')\n",
    "quality_checks(cur, conn, insert_checks)\n",
    "print('\\n' + 'insert_quality_checks...COMPLETE')\n",
    "\n",
    "conn.close()\n",
    "print('\\n' + 'End of ETL' + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "See 'data_dictionary.pdf' for details on columns and data-types. The create table queries can be found in 'sql_queries.py' under the 'CREATE TABLE QUERIES' header."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "**1. Rationale for the choice of tools and technologies for the project.**\n",
    "* For performance, I went with loading all data to S3, then ingesting into RedShift using the same AWS region.\n",
    "* Python and Spark were used to locally process and upload to S3.\n",
    "* RedShift was used to read from S3, clean, and process into the final fact & dimension tables.\n",
    "\n",
    "**2. How often the data should be updated and why.**\n",
    "* The data would be processed on a monthly basis to account for new i94 Immigration and weather data.\n",
    "\n",
    "**3. How to approach the problem differently under the following scenarios:**\n",
    "* The data was increased by 100x.\n",
    "    * RedShift would be able to handle this increase with more nodes to better utilize RedShift features of: Massively parallel processing, Columnar data storage, Data compression, Query optimizer, Result caching, Compiled code.\n",
    "* The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "    * To avoid daily table truncate and refreshing, we would convert the ETL and model into a slowly changing dimension pipeline and only capture new data or existing data changes (see https://en.wikipedia.org/wiki/Slowly_changing_dimension for details).\n",
    "* The database needed to be accessed by 100+ people.\n",
    "    * The data would need to be moved to an in-memory database such as Spark to avoid read/write performance issues that are inherent with traditional databases including RedShift. We would still utilize S3 and RedShift to store the data, but it would be staged to Spark where it would be accessed by users quickly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
